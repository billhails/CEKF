# CEKF Version 2 - Bytecode

Benchmarks so far have been encouraging, the `fib(35)` test with `-O2` now
takes around 5.5 seconds, but `fib(40)` still takes around 54 seconds,
while Nystrom's stack-based bytecode interpreter can do that calculation
in around 5 seconds. Of course this is due to using environments instead of
a stack, and walking trees instead of stepping through bytecode.

Another factor is that the entire AST needs to be protected, and must be
marked every time a garbage collection occurs.

So how difficult would it be to convert the AST to bytecode and use
a local stack? It turns out to be not so hard. Of course we still need
environments, because of closures that capture them, and it's quite
possible that version 2 will actually be **slower** initially, but
I'll discuss a possible version 2.1 later that I hope will fix that.

Anyway let's review the math from version 1 and see what the bytecode
equivalent might look like, starting with the evaluation of `aexp`.

The new machine no longer has any $\mathcal{A}$, $applyproc$ or
$applykont$ functions, as they are subsumed into the general $step$ function.
However the basic structure and discussion is the same.

I'll present the math for each step, then its bytecode equivalent.

## Internal Iterations

These first few cases are the equivalent of the old $\mathcal{A}$ function,
the interpreter is stepping through the bytecode encountering these expressions
and not changing the overall machine state, just the stack.

### Variables

Variables get looked up in the environment:

$$
\mathcal{A} (\mathtt{var}, \rho) = \rho(\mathtt{var})
$$


bytecode for that:

| bytecode | action |
|----------|--------|
`\| VAR \| frame \| offset \|` | `push(lookup(frame, offset, env))` |


The bytecode consiste of three bytes, a `VAR` tag that identifies that a variable is coming
up, then a byte for its frame and a byte for its offset in the frame (see [Lexical
Addressing](LEXICAL_ADDRESSING.md) for details if you haven't already.)

On seeing that, the bytecode interpreter does the lookup, and pushes the result onto the stack.

`env` here is the $\rho$ argument to the old $\mathcal{A}$ function.

Note that evaluating an `aexp` always has a stack cost of 1, e.g. there
is always one more element on the stack after evaluation an `aexp`.

### Constants

Constants evaluate to their value equivalents:

$$
\begin{align}
\mathcal{A}(\mathtt{integer}, \rho) &= z
\\
\mathcal{A}( \mathtt{\\#t}, \rho) &=  \mathbf{\\#t}
\\
\mathcal{A}( \mathtt{\\#f}, \rho) &=  \mathbf{\\#f}
\end{align}
$$

| bytecode | action |
|----------|--------|
| `\| INT \| hi \| lo \|` | `push(hi << 8 \| lo);` |
| `\| TRUE \|` | `push(1);` |
| `\| FALSE \|` | `push(0);` |

(16 bit integers for now).

### Lambdas

Lambdas become closures:

$$
\mathcal{A}(\mathtt{lam}, \rho) = \mathbf{clo}(\mathtt{lam}, \rho)
$$

| bytecode | action |
|----------|--------|
| `\| LAM \| nvar \| exp \|` | `push(clo(nvar, addr(exp), env);` |

Where `addr(exp)` is the index of the `exp` in the bytecode array.

Note the absence of explicit variable names. Again because of
lexical addressing the only thing the closure needs to know is the size
of the environment.

### Primitives

Primitive expressions are evaluated recursively:

$$
\mathcal{A}(\mathtt{(prim\ aexp_1\ aexp_2)}, \rho) =
  \mathcal{O}(\mathtt{prim})(\mathcal{A}(\mathtt{aexp_1}, \rho),
    \mathcal{A}(\mathtt{aexp_2}, \rho))
$$

where

$$
\mathcal{O} : \mathtt{prim} \rightharpoonup ((Value \times Value) \rightharpoonup Value)
$$

It starts to get interesting here, if we rewrite this to Reverse Polish
Notation we can achieve the entire operation very simply:

| bytecode | action |
|----------|--------|
| `\| aexp1 \| aexp2 \| PRIM \|` | `push(O(PRIM)(pop(), pop()));` |

I should probably explain that.

Consider a primitive sequence like `2 + 3 * 4`. That will parse to
`2 + (3 * 4)` and the AST will look like:

![AST](parse-tree.png)

There are various ways to print out that tree, for example for each
(non-terminal) node, printing the left hand branch, then printing the
operation, then printing the right hand branch would recover the infix
notation we started with. Howver if instead we print the left-hand
branch, then the right-hand branch, then the operation, we end up with
reverse polish notation: `2 3 4 * +` which is exactly the order we need
to evaluate the expressions:

* push 2.
* push 3.
* push 4.
* pop the 3 and the 4, multiply them and push the result 12.
* pop the 2 and the 12, add them and push the result 14.

Note that the entire operation has a stack cost of 1, preserving
that invariant.

## State Changes

The rest of these situations change the overall state of the machine, corresponding to
$step$ returning a new state.

### Function calls

For function calls, `step` first evaluates the function,
then the arguments, then it applies the function:

$$
step(\mathtt{(aexp_0\ aexp_1\dots aexp_n)}, \rho, \kappa, f) = applyproc(proc,\langle val_1,\dots val_n\rangle, \kappa, f)
$$

where

$$
\begin{align}
proc &= \mathcal{A}(\mathtt{aexp_0}, \rho)
\\
val_i &= \mathcal{A}(\mathtt{aexp_i}, \rho)
\end{align}
$$

and

$$
applyproc : Value \times Value^* \times Kont \times Fail \rightharpoonup \Sigma
$$

is

$$
\begin{align}
applyproc( \mathbf{clo} (\mathtt{(lambda\ (var_1\dots var_n)\ body)}, \rho),\langle val_1\dots val_n\rangle, \kappa, f) &=
(\mathtt{body}, \rho', \kappa, f)
\\
applyproc( \mathbf{cont}(\kappa'), \langle val \rangle, \kappa, f) &= applykont(\kappa', val, f)
\end{align}
$$

where

$$
\rho' = \rho[\mathtt{var_i} \Rightarrow val_i]
$$


| bytecode | condition | new state |
|----------|-----------|--------|
| `\| aexp1 \| ... \| aexpn \| aexp \| APPLY \|` | `clo(nargs, addr, env) = pop();` | `(addr, extend(env, pop(nargs)), K, F)` |
|  | `cont(letk(body, env, k')) = pop();` | `(body, extend(env, pop()), k', F)` |
| | `cont(halt) = pop();` | `(DONE, env, halt, F)` |

This is a bit more tricky. a sequence of `aexpn` arguments followed by a callable `aexp` followed by an `APPLY` token.
By the time we get to the `APPLY` all the `aexpx` have been evaluated and are sitting on the stack, with the
callable on top. But there are three possibilities. the callable could be a closure,
or it could be a continuation created by `call/cc`. If it's a continuation it could be a `letk` or a `halt`.
We deal with each separately.

The pseudocode `pop(nargs)` just means to copy the stack to the environment, a simple `memcpy` followed by a stack reset should suffice.

### Return

When the expression under evaluation is an `aexp`, that means we need
to return it to the continuation:

$$
step(\mathtt{aexp}, \rho, \kappa, f) = applykont(\kappa, val, f)
$$

where

$$
val = \mathcal{A}(\mathtt{aexp}, \rho)
$$

and

$$
applykont: Kont \times Value \times Fail \rightharpoonup \Sigma
$$

is

$$
\begin{align}
applykont(\mathbf{letk}(\mathtt{var}, \mathtt{body}, \rho, \kappa), val, f) &= (\mathtt{body}, \rho', \kappa, f)
\\
applykont(\mathbf{halt}, val, f) &= (\mathtt{DONE}, [], \mathbf{halt}, f)
\end{align}
$$

where

$$
\rho' = \rho[\mathtt{var} \Rightarrow val]
$$

| bytecode | condition | new state |
|----------|-----------|--------|
| `\| aexp \| RETURN \|` | `letk(body, env, k') = K;` | `(body, extend(env, pop()), k', F)` |
|  | `halt = K;` | `(DONE, env, halt, F)` |

We'll need that `RETURN` code to stop the internal iterations from proceeding.
There are two possibilities here, the continuation `K` could be a `letk` or a `halt`.
we deal with each.
